sim_name=13bit-Seed105
# Device and Seed
device=cuda:2
seed=105
# Arithmetic Processing from {float,lns}
arithmetic=lns
# Dataset from {fmnist,cifar10,cifar100,tinyimagenet,imagenet}
dataset=cifar100
# Network Architecture from {VGG11, VGG16,ResNet18,CifarResNet18} (check net_arch.py for definitions)
network=VGG11
# Training Hyper-parameters
epochs=50
batchsize=128
# Optimizer from {sgd,adam}
optim=sgd
lr=0.01
mom=0.9
# Scheduler
T_0=50
T_mult=1
eta_min=0.001
# LNS Fixed Point Representation
TOTAL_BITS=13
FRAC_BITS=5
# LNS Addition Approximation (0="uniform LUT", 1="Piece-wise Linear") and Technique (0="Sequential Block Adder", 1="Tree Adder"
QUANTIZAION_AWARE_APPROX=1
SAMPLE_MAGNITUDE=1
sum_approx=1
sum_technique=1
# Piece-wise Linear Approximation
LIN_SIZE=16
X_UB=12
# Unfiorm LUT Approximation
DPLUS_LUT_SIZE=64
DPLUS_DMAX=16
DMINUS_LUT_SIZE=64
DMINUS_DMAX=16
# Exponential Lookup Table (for Softmax)
EXP_LUT_SIZE=256
# TODO: Additional Hyper-Parameters: IGNORE FOR NOW 
loss_scale=1
weight_decay=0.0001
leaky_flag=0 
leaky_coeff=1e-3
